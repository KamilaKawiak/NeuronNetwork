# -*- coding: utf-8 -*-
"""MLP Analiza full.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NMIBNbOK1FgjG1bxAV2jShkDj9ThIT-4

#Analiza doboru hiperparametrow dla perceptronu wielowarstwowego z warstwa odrzucenia (dropout) dla problemu klasyfikacji obrazu na przykladzie zbioru Fashion MNIST. Badanie dotyczy wplywu roznych poziomow odrzucenia, metod inicjalizacji i liczby neuronow w warstwach ukrytych.  



Projekt zawiera kod pobierajacy dane i wykonujacy ich wstepne przetwarzanie. Nastepnie prezentujemy jedna architekture sieci neuronowej (MLP) i wykonujemy kilka treningow (na kilka sposobow. Wyniki zbieramy i omawiamy. Pokazujemy tez jak taka siec dziala i jakie daje wyniki (zgodnie z przyjetymi metrykami).

## FASHION MNIST Wczytanie i przygotowanie danych graficznych


https://www.tensorflow.org/datasets/catalog/fashion_mnist
https://github.com/zalandoresearch/fashion-mnist
"""

import tensorflow as tf

#Ładowanie danych
fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

#podzial na dane treningowe, walidacyjne i testowe
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=23)

# Normalizacja - minmax - [0, 255]
X_train = X_train/255
X_valid = X_valid/255
X_test = X_test/255

# Wymiary
#(45000 zdjec, 28 H, 28 W)
X_train.shape

#etykiety w postaci liczb 0-9. Znaczenie podano poniżej
y_train

"""#
0	T-shirt/top
1	Trouser
2	Pullover
3	Dress
4	Coat
5	Sandal
6	Shirt
7	Sneaker
8	Bag
9	Ankle boot

## Opis problemu

Dla powyższych danych o 10 kategoriach trenujemy **klasyfikator** w postaci sieci neuronowej typu **MLP**. Będziemy poszukiwać najlepszego modelu dla wybranej architektury widocznej poniżej (dwie ukryte warstwy gęste rozdzielonę warstwą dropout). Najlepszego modelu poszukujemy rozważając 3 hiperparametry: poziomow **odrzucenia**, metod **inicjalizacji** i **liczby neuronow** w warstwach ukrytych.

Powyższe parametry nie są najczęściej rozważanymi w projektach studenckich lub opracowaniach dostępnych w otwartej literaturze jak współczynnik uczenia, liczba warstw ukrytych czy metoda optymalizacji. Stąd pomysł na tego typu analizę.

## Sposób rozwiązania i oprogramowanie

Do przygotowania modelu MLP używamy obiektu Sequential biblioteki keras przeznaczonego do budowy sekwencyjnych sieci typu Feed Forward (bez cykli, wielu warstw wejściowych lub wyjściowych czy połączeń rezidualnych).

#Budowa modelu MLP
"""

from tensorflow.keras.layers import Dense, Flatten, Dropout

def create_model(dropout_rate = 0.2, init_mode='uniform', hidden_neurons=100):

  model = tf.keras.models.Sequential([
                                      Flatten(input_shape=[28, 28]),
                                      Dense(3* hidden_neurons, activation='relu', kernel_initializer=init_mode ),
                                      Dropout(dropout_rate),
                                      Dense(hidden_neurons, activation='relu', kernel_initializer=init_mode),
                                      Dense(10, activation='softmax')
  ])

  #kompilacja modelu
  #sparse - bo y to licza od 0 do 9
  #categorical_crossentropy - Klasyfikacja > 3 klasy

  #optimizer Adam - adaptacyjny - learning_rate

  #accuracy = stosunek poprawnych wynikow do wszystkich
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer='adam', metrics=['accuracy'])

  return model

my_model = create_model()

"""###Schemat modelu"""

tf.keras.utils.plot_model(my_model, show_shapes=True)

"""### Pierwszy Trening"""

#epoch - max liczba epok - trening raczej zakonczy sie wczesniej (Early Stopping)
#batch size - brac jak najwiekszy najwyzej sie wywali

#Jezeli algorytm nie widzi poprawy przez patience:int epok to konczy trening
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6)
history = my_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=50, batch_size=64, callbacks=[early_stopping], steps_per_epoch=100)

results = my_model.evaluate(X_test, y_test)

"""# Wizualizacja wyników

"""

import pandas as pd
import matplotlib.pyplot as plt

pd.DataFrame(history.history).plot(figsize=(8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

#loss, accuracy
results

"""Powyższy wykres ukazuje stabilny trening dla domyślnych hiperparametrów. Postaramy się uzyskać wyższe accuracy niż początkowe ~86%

#Poszukiwanie hiperparametrow

Teraz szukamy optymalnego zestawu hiperparametrów dla problemu. Zbadamy wpływ liczby neuronów, poziomu odrzucenia i metody inicjalizacji wag.
"""

def train_model(hyperparameters = {}):
  '''Fukcja wykonująca trening + ewaluację'''

  my_model = create_model(**hyperparameters)

  print("Training for ", hyperparameters)

  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6)
  history = my_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=50, batch_size=64, callbacks=[early_stopping], steps_per_epoch=100)

  results = my_model.evaluate(X_test, y_test)

  return results[1], my_model, history



"""#Analiza dropout"""

dropout_rates = [ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]

drop_results = {'dropout_rate':[], 'accuracy': []}


for dropout_rate in dropout_rates:
  accuracy, _ , _= train_model({'dropout_rate':dropout_rate})

  #zapisujemy wyniki
  drop_results['dropout_rate'].append(dropout_rate)
  drop_results['accuracy'].append(accuracy)

pd.DataFrame(drop_results)

"""Przy tak małej sieci (Tylko 3 warstwy) Droput nie ma wikszego wpywu na wyniki. Do dalszej pracy ustalimy droput na poziomie 20%.

**pogrubiony tekst**# Inicjalizacja Wag
"""

init_modes=['uniform','glorot_uniform', 'he_normal', 'lecun_normal', 'random_normal']

init_results = {'init_mode':[], 'accuracy': []}


for init_mode in init_modes:
  accuracy, _, _ = train_model({'init_mode':init_mode})

  init_results['init_mode'].append(init_mode)
  init_results['accuracy'].append(accuracy)

pd.DataFrame(init_results)

"""Brak wiekszych roznic pomiędzy treningami.

Różnic powinismy szukać w tempie zbieznosci algorytmu bardziej niż we wpływie na wyniki. 

*Random_normal* odpowiada za najdluzej trwający trening.
Dla dalszej analizy: *he_normal*

# Liczba neuronów w wartwach ukrytych
"""

numbers = [ 20, 50, 75, 100, 150, 200]

neurons_results =  {'hidden_neurons':[], 'accuracy': []}
models = []

for hidden_neurons in numbers:
  accuracy, model, _ = train_model({'dropout_rate': 0.2, 'init_mode':'he_normal', 'hidden_neurons':hidden_neurons})

  neurons_results['hidden_neurons'].append(hidden_neurons)
  neurons_results['accuracy'].append(accuracy)

  model.summary()
  

pd.DataFrame.from_dict(neurons_results)

"""Bardzo trudno ustalić prostą zależność pomiędzy liczbą neuronów a accuracy. Powtórzymy trening dla 200 neuronów i pozostałych najlepszych hiperparametrów.

# Najlepszy model - retraining
"""

accuracy, model, _ = train_model({'init_mode':'he_normal' , 'hidden_neurons':200, 'dropout_rate': 0.2,})
accuracy

results = model.evaluate(X_test, y_test)

pd.DataFrame(history.history).plot(figsize=(8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

#loss, accuracy
results

"""## Wyniki i wnioski

Wszystkie powyższe treningi przebiegły stabilnie. Nie zaobserwowaliśmy nadmiernego przeuczenia modeli (dużych błędów generalizacji), eksplodujących ani zanikających gradientów. Co ciekawe, wyniki dla różnych hiperparametrów nie różniły się znacząco, co potwierdza fakt, że nie są to najważniejsze hiperparametry do dobrania w procesie uczenia sieci neuronowej.

Brak problemów ze zbieżnością algorytmu wstecznej propagacji wynika ze stosunkowo małych rozmiarów sieci (jedynie dwie warstwy ukryte) oraz zastosowanej regularyzacji w postaci warstwy Dropout.

Osiągnięty wynik jest daleki od wyników deklarowanych dla tego zbioru w literarturze (powyżej 99% [źródło](https://paperswithcode.com/sota/image-classification-on-mnist)). Nie jest to jednak wynik niski, biorąc pod uwagę występowanie 10 różnych klas. Model oparty na przewidywaniu dominanty zbioru osiąga accuracy ok. 10%.
"""